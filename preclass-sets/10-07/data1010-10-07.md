---
jupyter:
  jupytext:
    formats: ipynb,md
    text_representation:
      extension: .md
      format_name: markdown
      format_version: '1.2'
      jupytext_version: 1.6.0
  kernelspec:
    display_name: Julia 1.5.0
    language: julia
    name: julia-1.5
---

## Simulation techniques and kernel density estimation

#### *07 Octorber 2019*
#### *DATA 1010*


# Simulation


In this section, we will learn techniques for sampling from probability distributions. This is a useful skill because it allows us to flexibly generate data for which we know the underlying probability distribution. These data can be used to test machine learning methods and see how well they perform in ideal circumstances. We can adjust the number of observations, the shape of the distribution, etc., and see how these changes affect the results.


Perhaps the most important simulation technique is the **inverse CDF trick**. It gives us a way to simulate from any probability distribution on the number line whose cumulative distribution function we can compute with.

Given a cumulative distribution function $F$, let us define the **generalized inverse** $F^{-1}: [0,1] \to [-\infty,\infty]$ so that $F^{-1}(u)$ is the left endpoint of the interval of points which are mapped by $F$ to a value which is greater than or equal to $u$.

The generalized inverse is like the inverse function of $F$, except that if the graph of $F$ has a vertical jump somewhere, then all of the $y$ values spanned by the jump get mapped by $F^{-1}$ to the $x$-value of the jump, and if the graph of $F$ is flat over a stretch of $x$-values, then the corresponding $y$-value gets mapped by $F^{-1}$ back to the left endpoint of the interval of $x$ values.

The inverse CDF trick says that if $U$ is uniformly distributed on $[0,1]$, then the cumulative distribution of $X = F^{-1}(U)$ is $F$.


## Problem 1
Apply the inverse CDF trick to simulate a random variable whose distribution is $f(x) = 2x$ on the interval $[0,1]$. Plot a histogram of many values returned by this function, and show that the histogram has the same approximate shape as the graph of $f$.

```julia

```

## Problem 2

Use the inverse CDF idea to draw from the distribution on $[0,1]^2$ whose density function is $f(x,y) = \frac32(x^2 + y^2)$.

Hint: first draw $X$ from its distribution, then draw $Y$ from its conditional distribution given the sampled value of $X$.

```julia

```

## Problem 3

Write a function which starts with `U = rand()` and uses `U` to return a random integer which is 1 with probability 3/5,  2 with probability 1/5, and 3 with probability 1/5. 

```julia

```

---

### Introduction to statistics

The central problem of statistics is to make inferences about a population or data-generating process based on the information in a finite sample drawn from the population.

**Parametric estimation** involves an assumption that distribution of the data-generating process comes from a family of distributions parameterized by finitely many real numbers, while **nonparametric estimation** does not.

## Problem 4

Give an example each of parametric and non-parametric estimation methods?


*Write your solution here*





## Problem 5

<img src="exam-density.png" style="float: right; width: 40%">

Find the conditional expectation of $Y$ given $X$ if 

$$
f(x,y) = \frac{3}{4000(3/2)\sqrt{2\pi}}x(20-x)e^{-\frac{1}{2(3/2)^2}\left(y - 2 - \frac{1}{50}x(30-x)\right)^2}
$$


## Problem 6

<img src="kde-figures.png" style="float: right; width: 40%">

Which of the $\lambda$ values shown does the best job of approximating the density, in your opinion?

```julia

```

<img src="kde_slice.png" alt="p" width="400" align=right>

### Problem 7

How many of the samples in the figure shown have a nonzero contribution to the integral representing the conditional expectation of $Y$ given $X = x$? (The heatmap shows the joint density of $X$ and $Y$.)


*Write your solution here*





---

### Problem 8

Suppose we have a probability density function $f$ on a rectangle in $\mathbb{R}^2$, and we compute its values on a fine-mesh grid of points along a vertical line at position $x$ through the rectangle and store those values in a vector `v`. Suppose that the $y$-coordinates of the grid points are stored in a vector `ys`. 

Write a line of Julia code to approximate the conditional expectation of $Y$ given $X = x$, if $(X,Y)$ has PDF $f$.


*Write your solution here*





---

Recall that $K_\lambda(x,y) = D_{\lambda}(x)D_\lambda(y)$ where $D_\lambda(x) = \frac{70}{81\lambda}\left(1-\frac{|u|^3}{\lambda^3}\right)^3$.

<img src="kde_diagram.png" alt="p" width="350" align=right>

### Problem 9

Put the values $K_\lambda(x_1-x,y_1-y)$, $K_\lambda(x_2-x,y_2-y)$, $K_\lambda(x_3-x,y_3-y)$ in order from least to greatest.


*Write your solution here*




```julia

```
